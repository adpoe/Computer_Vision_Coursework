############################################
### @author Anthony (Tony)
### @email adp59@pitt.edu
### CS1674 - Computer Vision
### Assignment #09 - Writeup for Questions in Assignment Prompt
###   RESULTS.TXT
###   -  Lists how many bounding boxes predicted positive vs ground truth
############################################

    RESULTS
    - PREDICTED POSITIVE (Total): 1403  data points
    - PREDICTED NEGATIVE (Total): 17335 data points

    GROUND TRUTH COUNTS (from 15 total images):
    - From test set:
        * Image 1 = 0 people
        * Image 2 = 0 People
        * Image 3 = 0 People
        * Image 4 = 0 people
        * Image 5 = 0 people
        * Image 6 = 1 person
        * Image 7 = 4 people
        * Image 8 = 8 people
        * Image 9 = 19 people (2 groups) --> call this 2, for simplicity
        * Image 10 = 5 people
        * Image 11 = 6 people
        * Image 12 = 6 people
        * Image 13 = 1 person
        * Image 14 = 1 person
        * Image 15 = 1 person
                    -------------
                    Total = 30 people (if we consolidate each group in image 9 into one data point)

    NOTES
    - Because the data set was so large, and there over 1000 positive matches, I decided to take a uniform sample from my data set of positive matches.
    - For the purpose of this exercise, my sample size is N=30, which is a high enough number to make some reasonable generalizations about the rest of my data.
    - Moreover, because I've identified 30 people in my "Ground Truth" count, within this data set --> this calculating precision and recall more simple mathematically.

    PRECISION
        * (Num images that **actually have** Ground Truth of Attribute) / (Total Predictions)
        * 17 / 30 = 0.5667

    RECALL
        * (Number of Indivial People Found) / (Total **Ground Truth** Count)
        * 19  / 30 = 0.6333

    COMMENTS
    - Again, because of the huge number of data points to evaluate, I took a uniform sampling, which was gathered programmatically (my code for this is viewable in this repo).
    - I made judgment calls in my counting, both for precision and recall, basing my decisions off of the (Area of Overlap) / (Area of Union) heuristic noted in the assignment sheet. I did not measure this programmatically, however, I used my personal judgment, with this principle as a guideline.
    - Because I took a sample, my intuition tells me that Recall especially, may be much higher in reality. Precision is likely accurate within some reasonable margin of error.
    - Also to note: I did not re-size the people in each of my images (in test set), as it was unclear how to do so. Because of this, I am identifying **parts** of people in some of my positives. When this happens, I am counting it as a whole person.


    Thanks,
    Tony

